## Error Estimation In Monte Carlo Methods

---
### Average Squared Error

The most basic concept of error estimation is Average Squared Error. It can be defined as follows
$$
\tag{1.1} 
\frac{\sigma^2}{n}
$$

It is often difficult to precisely evaluate $\sigma$, so we need to estimate it from the sample values. The most popular used estimates of $\sigma$ are

$$
\tag{1.2}
s^2 = \frac{1}{n - 1}\sum_{n=1}^{n}(Y_i - \hat{\mu_n})^2
$$

$$
\tag{1.3}
\hat{\sigma}^2 = \frac{1}{n}\sum_{n=1}^{n}(Y_i - \hat{\mu_n})^2
$$

With a large $n$, we can assume that $$E(s)=\sigma$$

---
### Confidence Interval

Another important concept that aids in estimating errors is the Confidence Interval. It offers an estimation of the interval in which the true value of a parameter of interest is likely to lie.

To understand how to compute a confidence interval, it is necessary to first introduce Standard Normal Distribution and the IID (independently and identically distributed) Central Limit Theorem.

##### Standard Normal Distribution

The standard normal distribution is the normal distribution with mean 0 and variance 1, which has the following probability density function

$$
\tag{1.4}
\varphi(t) = \frac{e^{-\frac{1}{2}t^2}}{\sqrt{2\pi}}, ~~~~for~~-\infty < t < \infty
$$

<img src="https://github.com/WeakKnight/weakknight.github.io/raw/master/assets/mc/standard_normal_distribution_pdf.png" class ="medium-image"/>

The cumulative distribution function is 

$$
\tag{1.5}
\Phi(x) = \int_{-\infty}^{x} \varphi(t) dt
$$

<img src="https://github.com/WeakKnight/weakknight.github.io/raw/master/assets/mc/standard_normal_distribution_cdf.png" class ="medium-image"/>

When T has the standard normal distribution, we write it as $T \backsim \mathcal{N}(0, 1)$

##### IID Central Limit Theorem
Let $Y_1, Y_2, ..., Y_n$ be independent and identically distributed random variables with mean $\mu$ and finite variance $\sigma^2 > 0$. Let $\hat{\mu_n}=\frac{1}{n}\sum_{n=1}^{n}{Y_i}$. Then for all $z\in\mathbb{R}$
$$
\tag{1.6}
\mathbb{P}(\sqrt{n}\frac{\hat{\mu_n}-\mu}{\sigma} \le z) \to \Phi(z)
$$

We also need Slutsky's Theorm for safely plugging estimated variables.

##### Slutskyâ€™s Theorem
Suppose that random variables $Y_n \xrightarrow{d} Y$ and $Z_n \xrightarrow{d} \tau$. Then $Y_n + Z_n \xrightarrow{d} Y + \tau$ and $Y_n Z_n \xrightarrow{d} \tau Y$. If $\tau \ne 0$ then $Y_n/Z_n \xrightarrow{d} Y/\tau$.

Now we have enough facilities to derive what we want.

Assuming $\Delta > 0$, we can write the probability that a random variable is outside the confidence intervel as follows, 

$$
\mathbb{P}(|\hat{\mu_n}-\mu| \ge \frac{\Delta\sigma}{\sqrt{n}}) 
$$

Which correspond to the following confidence interval

$$
 \tag{1.7}
 \hat{\mu_n} - \frac{\Delta \sigma}{\sqrt{n}} \le \mu \le  \hat{\mu_n} + \frac{\Delta \sigma}{\sqrt{n}}
$$

And because $\sigma \xrightarrow{d} s$,

$$
\mathbb{P}(|\hat{\mu_n}-\mu| \ge \frac{\Delta\sigma}{\sqrt{n}})  \xrightarrow{d} \mathbb{P}(|\hat{\mu_n}-\mu| \ge \frac{\Delta s}{\sqrt{n}}) 
$$

Then we have

$$
\mathbb{P}(|\hat{\mu_n}-\mu| \ge \frac{\Delta s}{\sqrt{n}}) = \mathbb{P}(\sqrt{n}\frac{\hat{\mu_n}-\mu}{s} \ge \Delta)+ \mathbb{P}(\sqrt{n}\frac{\hat{\mu_n}-\mu}{s} \le -\Delta)
$$

Based on $1.6$,

$$
\mathbb{P}(\sqrt{n}\frac{\hat{\mu_n}-\mu}{s} \ge \Delta) \to 1 - \Phi(\Delta)
$$

$$
\mathbb{P}(\sqrt{n}\frac{\hat{\mu_n}-\mu}{s} \le -\Delta) \to \Phi(-\Delta)
$$

Because $\varphi(\Delta) = \varphi(-\Delta)$ and $\Phi(\infty) = 1$,
$$
\Phi(\Delta) + \Phi(-\Delta) = 1
$$

Then we have

$$
\tag{1.8}
\mathbb{P}(|\hat{\mu_n}-\mu| \ge \frac{\Delta s}{\sqrt{n}}) \to 2\Phi(-\Delta) 
$$

With $1.7$ and $1.8$, we can derive confidence interval with different coverage chances. For example, if we want to have a confidence interval with 99% coverage chance, based on $1.8$ we can have

$$
2\Phi(-\Delta) = 0.01
$$

$$
\Phi(\Delta) = 0.995
$$

$$
\Delta = \Phi^{-1}(0.995) \approx 2.58
$$

Finally, based on $1.7$, we can write the confidence interval with 99% coverage chance as follows,

$$
\hat{\mu_n} - \frac{2.58 s}{\sqrt{n}} \le \mu \le  \hat{\mu_n} + \frac{2.58 s}{\sqrt{n}}
$$

For coverage chance $\alpha$, we have the confidence interval as follows,

$$
\tag{1.9}
\hat{\mu_n} \pm \frac{\Phi^{-1}(1 - \alpha / 2) s}{\sqrt{n}}
$$

However, our definition of confidence interval is still in an estimated form. For a small n, it is better to use the Student's t-distribution instead of the standard normal distribution. Then we can have

$$
\tag{1.10}
\hat{\mu_n} \pm {t_{(1 - \alpha/2, n-1)}} s / \sqrt{n}
$$

In realtime rendering, we often have a limited sample count like 8 or 16. Let us compute the confidence interval with coverage chance 99% using normal distribution and t-distribution. Assuming $n = 16$,

If using normal distribution,
$$
\hat{\mu_n} \pm  0.645s
$$

If using t-distribution,
$$
\hat{\mu_n} \pm  0.737s
$$

Using t-distribution, the confidence interval is roughly $28.5\%$ larger.

Furthermore, we usually use a moving average estimator for realtime rendering where normal distribution and t-distribution are not appliable. For moving average, we have confidence interval as follows

$$
\tag{1.11}
n s^2 / {\chi}_{n,\alpha / 2}^{2} \le \mu \le n s^2 / {\chi}_{n,1 - \alpha / 2}^{2}
$$

Assuming that $n = 16$ and the coverage chance is $99\%$,

$$
\chi_{16, 0.005}^{2} = 34.267
$$

$$
\chi_{16, 0.995}^{2} = 5.142
$$

then the confidence interval is

$$
(0.467s^2, 3.112s^2)
$$

---
### Gauranteed Confidence Interval

Until now, we have acquired the proficiency to calculate an estimated confidence interval. With further information, we can investigate the optimal sample size required to find a guaranteed confidence interval.

##### Hoeffding's Inequality

Let $Y_1 ..., Y_n$ be independent random variables with $\mathbb{P}(a_i \le Y_i \le b_i) = 1$ for $i = 1, ..., n$. Then for $\epsilon > 0$ 

$$
\tag{1.12}
\mathbb{P}(|\sum_i(Y_i - \mathbb{E}(Y_i))| \ge \epsilon) \le 2 e^{-2\epsilon^2 / \sum_{i=1}^{n}(b_i - a_i)^2}
$$

Based on $1.12$, given bounds $[a, b]$ for random variables, we can compute how large $n$ must be for a gauranteed confidence interval.

$$
Let~\epsilon = n \varepsilon / 2
$$

Then

$$
\mathbb{P}(|\sum_i(Y_i - \mathbb{E}(Y_i))| \ge \epsilon) = \mathbb{P}(|\sum_i(Y_i - \mathbb{E}(Y_i))| \ge n \varepsilon / 2)
$$

$$
\mathbb{P}(|\sum_i(Y_i - \mathbb{E}(Y_i))| \ge n \varepsilon / 2) = \mathbb{P}(\frac{1}{n}|\sum_i(Y_i - \mathbb{E}(Y_i))| \ge \varepsilon / 2)
$$

Then we have

$$
\mathbb{P}(|\hat{\mu} - \mu| \ge \varepsilon / 2) \le 2e^{-2(n \varepsilon / 2)^2 / \sum_{i=1}^{n}(b_i - a_i)^2}
$$

If we want a coverage chance $\delta$ for confidence interval $\hat{\mu} \pm \varepsilon / 2$

$$
\delta = 2e^{-2(n \varepsilon / 2)^2 / \sum_{i=1}^{n}(b_i - a_i)^2}
$$

Then 

$$
n = -log(\delta/2) 2(\sum_{i=1}^{n}(b_i - a_i)^2/n) / \varepsilon^2 
$$

Finally

$$
\tag{1.13}
n = \frac{2(b - a)^2 log(2/\delta)}{\varepsilon^2} 
$$

Based on $1.13$, if we have $Y_i \in [0, 100]$, and we want to know for confidence interval $\mu \pm 0.001$ with $99\%$ confidence,

$$
n = 2 \times {100}^2 log(2/0.01) / {0.001}^2 \approx 1.06 \times {10}^{11}
$$

For confidence interval $\mu \pm 0.1$ with $90\%$ confidence,
$$
n = 2 \times {100}^2 log(2/0.1) / {0.1}^2 = 1.06 \times {10}^11 \approx 5,991,465
$$

<!-- and 

$$
\mathbb{P}(\sum_i(Y_i - \mathbb{E}(Y_i)) \le -\epsilon) \le e^{-2\epsilon^2 / \sum_{i=1}^{n}(b_i - a_i)^2}
$$ -->

---
### Computing the standard error

Currently, we possess computational tools to determine the confidence interval. However, we are in need of a reliable methodology to calculate the variable $s$. Fortunately, we have a suitable algorithm at our disposal to address this requirement.

Let $S_n = \sum_{i=1}^{n}(y_i - \hat{\mu_n})^2$, $S_1 = 0$ and $\hat{\mu_1} = y_1$

$$
\delta_i = y_i - \hat{\mu_{i-1}}
$$

$$
\hat{\mu_i} = \hat{\mu_{i-1}} + \delta_i / i
$$

$$
S_i = S_{i - 1} + \frac{i - 1}{i}\delta_i^2
$$

$$
s^2 = S_n/(n - 1)
$$

<!-- If using moving average $\hat{\mu_n} =\lambda \hat{\mu_{n - 1}} + (1-\lambda)y_n$, 

$$
\delta_i = y_i - \hat{\mu_{i}}
$$

$$

$$ -->
---
### Variance Of Weighted Average 

We need take extra care for weighted average. It can be wrriten as

$$
\hat{\mu}_n = {\sum_{i=1}^{n} w_i Y_i}~, ~where~~\sum_{i=1}^{n} w_i = 1
$$

Its variance can be wrriten as

$$
\tag{1.14}
Var(\hat{\mu}_n) = {\sum_{i=1}^{n} w_i^2 (Y_i - \hat{\mu}_n)^2}
$$

---
### Application In Computer Graphics

Confidence intervals can be utilized for various applications such as Path Tracing with Adaptive Sampling or Image Reconstruction tasks like denoising and image upscaling.

Let us take a look at a straightforward implementation of adaptive sampling.

For pixel color $p_x$, and the accumulator is based on moving average with $n = 16$.

Moving average can be wrriten as 

$$
\hat{x}_n = \lambda x_n + (1 - \lambda)\hat{x}_{n-1}
$$

It can be generalized as a weighted average employing the subsequent weights

$$
w_0 = (1- \lambda)^n~,~for~~i = 0
$$

$$
w_i = \lambda(1-\lambda)^{n - i}~,~for~~i > 0
$$

Plug into $1.14$

$$
Var(\hat{x}_n) = (1- \lambda)^{2n}(x_0 - \hat{x}_n)^2 + {\sum_{i=1}^{n} \lambda^2(1-\lambda)^{2(n - i)} (x_i - \hat{x}_n)^2}
$$

$$
x_i = \frac{\hat{x}_i - (1 - \lambda)\hat{x}_{i-1}}{\lambda}
$$

$$
Var(\hat{x}_n) = (1- \lambda)^{2n}(x_0 - \hat{x}_n)^2 +{\sum_{i=1}^{n} \lambda^2(1-\lambda)^{2(n - i)} (\frac{\hat{x}_i - (1 - \lambda)\hat{x}_{i-1}}{\lambda} - \hat{x}_n)^2}
$$

Assume $\hat{x}_n \approx \hat{x}_{n - 1}$

$$
Var(\hat{x}_n) \approx Var(\hat{x}_{n - 1}) + \lambda^2 (\frac{\hat{x}_n - (1 - \lambda)\hat{x}_{n-1}}{\lambda} - \hat{x}_n)^2
$$

---
### References
> [Art B. Owen.(2013).Monte Carlo theory, methods and examples.](https://artowen.su.domains/mc/)<br>
> [Carol Alexander.(2001).Market Models.Chapter 5.](http://www.wiley.com/legacy/wileychi/marketmodels/chapter5.pdf)<br>

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>